{
  "acc": 0.925510013101254,
  "roc": 0.9631484416654991,
  "f1": 0.8148837209302326,
  "positive_fraction": 0.19015534344001497,
  "eval_loss": 0.189670960533204,
  "hypers": {
    "local_rank": -1,
    "global_rank": 0,
    "world_size": 1,
    "model_type": "bert-base-chinese",
    "model_name_or_path": "",
    "resume_from": ".\/datasets\/IM_TQA\/bert-base-chinese-epoch3-warmup0.1\/row_bert_base",
    "config_name": "",
    "tokenizer_name": "",
    "cache_dir": "",
    "do_lower_case": true,
    "gradient_accumulation_steps": 8,
    "learning_rate": 2e-5,
    "weight_decay": 0.01,
    "adam_epsilon": 1e-8,
    "max_grad_norm": 1.0,
    "warmup_instances": 0,
    "warmup_fraction": 0.1,
    "num_train_epochs": 3,
    "no_cuda": false,
    "n_gpu": 1,
    "seed": 1234,
    "fp16": false,
    "fp16_opt_level": "O1",
    "full_train_batch_size": 64,
    "per_gpu_eval_batch_size": 8,
    "output_dir": ".\/datasets\/IM_TQA\/bert-base-chinese-epoch3-warmup0.1\/row_bert_base",
    "save_total_limit": 1,
    "save_steps": 0,
    "use_tensorboard": false,
    "log_on_all_nodes": false,
    "server_ip": "",
    "server_port": "",
    "__required_args__": [],
    "max_seq_length": 512,
    "num_labels": 2,
    "single_sequence": false,
    "additional_special_tokens": "",
    "is_separate": false,
    "kd_alpha": 0.9,
    "kd_temperature": 10.0,
    "train_dir": ".\/datasets\/IM_TQA\/train_rows.jsonl.gz",
    "dev_dir": ".\/datasets\/IM_TQA\/test_rows.jsonl.gz",
    "train_instances": 29849,
    "hyper_tune": 0,
    "prune_after": 5,
    "save_per_epoch": false,
    "teacher_labels": "",
    "per_gpu_train_batch_size": 8,
    "stop_time": null
  }
}
